{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:62937\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [05/Dec/2022 19:43:15] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [05/Dec/2022 19:43:15] \"GET /static/style.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [05/Dec/2022 19:43:15] \"GET /style.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [05/Dec/2022 19:43:15] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 20:01:02.301168: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-12-05 20:01:02.371202: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "127.0.0.1 - - [05/Dec/2022 20:01:02] \"POST /get HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 0.8611093]]\n",
      "[{'intent': 'greeting', 'probability': '0.8611093'}]\n",
      "greeting\n",
      "Answer:  Hi there, how can I help?\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "can i apply to the program?\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Dec/2022 20:01:11] \"POST /get HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 0.895437]]\n",
      "[{'intent': 'admission', 'probability': '0.895437'}]\n",
      "admission\n",
      "Answer:  Please review the program admission requirements on our website https://analytics.georgetown.edu/admissions/requirements/\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "what is the class size?\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Dec/2022 20:01:26] \"POST /get HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0.9720329]]\n",
      "[{'intent': 'Class Size', 'probability': '0.9720329'}]\n",
      "Class Size\n",
      "Answer:  There are typically 100-120 students in each incoming class. However, please be assured that our class sizes remain small -- typically there are 25-35 students per classroom, so our students are ensured individual attention.\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "is gre mandatory?\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Dec/2022 20:01:51] \"POST /get HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 0.9959442]]\n",
      "[{'intent': 'GRE_req', 'probability': '0.9959442'}]\n",
      "GRE_req\n",
      "Answer:  Please be aware that we do not require the GRE to be submitted as a part of the application, it is completely optional. However, if you decide to submit scores, we do accept the GRE or GMAT or GRE at home scores.\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "may i get a fox?\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [05/Dec/2022 20:02:26] \"POST /get HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[{'intent': 'failed', 'probability': '1'}]\n",
      "failed\n",
      "Answer:  Sorry, I don't understand that. Please rephrase/ask another question or send an email to gradanalytics@georgetown.edu\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from flask import Flask, render_template, request\n",
    "import nltk\n",
    "from keras.models import load_model\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import random\n",
    "\n",
    "random.seed(2022)\n",
    "\n",
    "\n",
    "# chat initialization\n",
    "model = load_model(\"model_output/chatbot_model.h5\")\n",
    "intents = json.loads(open(\"data/intents.json\").read())\n",
    "words = pickle.load(open(\"model_output/words.pkl\", \"rb\"))\n",
    "classes = pickle.load(open(\"model_output/classes.pkl\", \"rb\"))\n",
    "\n",
    "app = Flask(__name__)\n",
    "# run_with_ngrok(app) \n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index_v2.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/get\", methods=[\"POST\"])\n",
    "def chatbot_response():\n",
    "    msg = request.form[\"msg\"].lower()\n",
    "    print(msg)\n",
    "    ints = predict_class(msg, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res\n",
    "\n",
    "\n",
    "# chat functionalities\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0] * len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(\"found in bag: %s\" % w)\n",
    "    return np.array(bag)\n",
    "\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "   \n",
    "    ERROR_THRESHOLD = 0.5\n",
    "\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    #sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(results)\n",
    "    \n",
    "\n",
    "    confidence_probability=0.6\n",
    "    return_list = []\n",
    "\n",
    "    if len(results) == 0:\n",
    "        return_list.append({\"intent\": 'failed', \"probability\": '1'})\n",
    "    elif results[0][1] >=  confidence_probability:\n",
    "        return_list.append({\"intent\": classes[results[0][0]], \"probability\": str(results[0][1])})\n",
    "    else: \n",
    "        return_list.append({\"intent\": 'failed', \"probability\": '1'})\n",
    "   \n",
    "    print(return_list)\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0][\"intent\"]\n",
    "    print(tag)\n",
    "    \n",
    "    list_of_intents = intents_json[\"intents\"]\n",
    "\n",
    "    for i in list_of_intents:\n",
    "        if i[\"tag\"] == tag:\n",
    "            result = random.choice(i[\"responses\"])\n",
    "            break\n",
    "        else:\n",
    "            result= \"Sorry, I don't understand that. Please rephrase/ask another question or send an email to gradanalytics@georgetown.edu\"\n",
    "        \n",
    "    print(\"Answer: \", result)\n",
    "    print(\"-----------------------------------------------------------------------------------------------------\")\n",
    "    print(\"\\n\")\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port='0000')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "170ce1dd6f4b41f87201bcaa71a63fa2dc3233be29ef0b52e1889288dbb460d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
